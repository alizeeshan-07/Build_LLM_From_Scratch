name: ‚ùì Question/Help
description: Ask a question or get help with the code
title: "[QUESTION] "
labels: ["question", "help-wanted"]
body:
  - type: markdown
    attributes:
      value: |
        Have a question about the code or LLM concepts? We're here to help! 
        
        **Before asking**, please check:
        - üìö [Learning Path](../docs/learning_path.md)
        - üîç [Existing Issues](../../issues)
        - üìñ [README](../../README.md)
        - üîß [Troubleshooting Guide](../docs/troubleshooting.md)

  - type: textarea
    id: question
    attributes:
      label: ‚ùì Your Question
      description: What would you like to know?
      placeholder: "I don't understand how the self-attention mechanism works in module 03. Specifically, I'm confused about..."
    validations:
      required: true

  - type: dropdown
    id: topic
    attributes:
      label: üìö Topic Area
      description: What topic is your question about?
      options:
        - Tokenization Concepts
        - Embeddings/Vector Representations
        - Attention Mechanisms
        - Transformer Architecture
        - Model Training
        - Text Generation/Inference
        - Fine-tuning
        - Mathematics/Theory
        - Code Implementation
        - Project Setup
        - Performance/Optimization
        - Debugging
        - Other
    validations:
      required: true

  - type: textarea
    id: context
    attributes:
      label: üîç Context
      description: What have you tried? What's confusing you?
      placeholder: |
        I've read the module 3 documentation and tried running the attention example, but I'm confused about:
        - How the query, key, value matrices are computed
        - Why we divide by sqrt(d_k) in the attention calculation
        
        I've tried looking at the code in self_attention.py but...

  - type: textarea
    id: code
    attributes:
      label: üíª Relevant Code
      description: If applicable, share the code you're working with
      placeholder: |
        ```python
        def scaled_dot_product_attention(query, key, value):
            # This part confuses me
            scores = torch.matmul(query, key.transpose(-2, -1))
            # Why divide by sqrt(d_k)?
            scores = scores / math.sqrt(query.size(-1))
        ```
      render: python

  - type: dropdown
    id: level
    attributes:
      label: üìä Experience Level
      description: This helps us tailor our response
      options:
        - Beginner (New to ML/Deep Learning)
        - Intermediate (Some ML experience)
        - Advanced (Experienced with ML/NLP)
        - Expert (Professional ML/Research background)

  - type: checkboxes
    id: help_type
    attributes:
      label: üéØ What kind of help are you looking for?
      options:
        - label: Conceptual explanation (help me understand the theory)
        - label: Code walkthrough (explain how the implementation works)
        - label: Mathematical details (show me the math behind it)
        - label: Debugging assistance (help me fix broken code)
        - label: Learning resources (point me to good references)
        - label: Implementation suggestions (how should I approach this?)
        - label: Best practices (what's the right way to do this?)

  - type: textarea
    id: attempted_solutions
    attributes:
      label: üîß What Have You Tried?
      description: What solutions or approaches have you already attempted?
      placeholder: |
        - Tried reading the original Transformer paper
        - Looked at other implementations online
        - Asked on Stack Overflow but didn't get clear answers
        - Tried debugging with print statements