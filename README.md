# ğŸ¤– Build Large Language Model from Scratch

A comprehensive implementation following the book's methodology, with clear chapter-by-chapter progression.

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## ğŸ“š About This Repository

This repository contains a complete implementation of a Large Language Model (LLM) built from scratch, following best practices for educational purposes. Each chapter builds upon the previous one, creating a fully functional GPT-style model.

## ğŸ¯ Learning Path

This repository is organized into progressive modules. Here's the recommended learning sequence:

1. **Module 01: Tokenization** (`src/modules/01_tokenization/`)
2. **Module 02: Embeddings** (`src/modules/02_embeddings/`)
3. **Module 03: Attention** (`src/modules/03_attention/`)
4. **Module 04: Transformer Blocks** (`src/modules/04_transformer_blocks/`)
5. **Module 05: Gpt Model** (`src/modules/05_gpt_model/`)
6. **Module 06: Training** (`src/modules/06_training/`)
7. **Module 07: Inference** (`src/modules/07_inference/`)
8. **Module 08: Fine Tuning** (`src/modules/08_fine_tuning/`)

## ğŸš€ Quick Start

```bash
# Clone the repository
git clone https://github.com/alizeeshan-07/Build_LLM_From_Scratch.git
cd Build_LLM_From_Scratch

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Run a quick demo
python examples/quick_start.py
```

## ğŸ“– Module Details

## Module 01: Tokenization

ğŸ“ **Location**: `src/modules/01_tokenization/`

## Module 02: Embeddings

ğŸ“ **Location**: `src/modules/02_embeddings/`

## Module 03: Attention

ğŸ“ **Location**: `src/modules/03_attention/`

## Module 04: Transformer Blocks

ğŸ“ **Location**: `src/modules/04_transformer_blocks/`

## Module 05: Gpt Model

ğŸ“ **Location**: `src/modules/05_gpt_model/`

## Module 06: Training

ğŸ“ **Location**: `src/modules/06_training/`

## Module 07: Inference

ğŸ“ **Location**: `src/modules/07_inference/`

## Module 08: Fine Tuning

ğŸ“ **Location**: `src/modules/08_fine_tuning/`

## ğŸ› ï¸ Utilities

Common utilities used across all chapters:

### `visualization.py`
Visualization utilities for plotting attention weights, training curves, etc.

- `plot_attention_weights()`: Plot attention weights as a heatmap
- `plot_training_curves()`: Plot training and validation loss curves
- `plot_token_embeddings()`: Plot token embeddings in 2D using dimensionality reduction

### `cli.py`
Command-line interface for the Build LLM project.
Provides easy access to common operations.

- `train_model()`: Train a model with specified configuration
- `generate_text()`: Generate text using a trained model
- `tokenize_text()`: Tokenize input text and display results
- `main()`: Main CLI entry point

### `data_utils.py`
Data processing utilities for loading and preprocessing text data.

- `load_text_data()`: Load text data from a file
- `preprocess_text()`: Basic text preprocessing
- `create_vocab()`: Create vocabulary from list of texts

### `metrics.py`
Evaluation metrics for language models.

- `calculate_perplexity()`: Calculate perplexity from cross-entropy loss
- `calculate_accuracy()`: Calculate token-level accuracy
- `evaluate_model()`: Evaluate model on a dataset

### `init.py`
Utility functions and helpers for the Build LLM project.

## ğŸ“„ Project Structure

```
src/
â”œâ”€â”€ modules/
â”‚   â”œâ”€â”€ 01_tokenization/           # Text tokenization methods
â”‚   â”œâ”€â”€ 02_embeddings/             # Word and positional embeddings
â”‚   â”œâ”€â”€ 03_attention/              # Attention mechanisms
â”‚   â”œâ”€â”€ 04_transformer_blocks/     # Transformer architecture
â”‚   â”œâ”€â”€ 05_gpt_model/              # Complete GPT model
â”‚   â”œâ”€â”€ 06_training/               # Training procedures
â”‚   â”œâ”€â”€ 07_inference/              # Text generation
â”‚   â””â”€â”€ 08_fine_tuning/            # Model fine-tuning
â””â”€â”€ utils/                         # Common utilities
```

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request. Make sure to follow the existing code structure and add appropriate documentation.

## ğŸ“§ Contact

Feel free to reach out if you have questions about the implementation or want to discuss LLM concepts!

---
*ğŸ“ This README is automatically updated when Python files are modified.*
