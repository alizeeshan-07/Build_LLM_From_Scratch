# ğŸ¤– Build Large Language Model from Scratch

A comprehensive implementation following a modular, educational approach to understanding and building Large Language Models (LLMs) from the ground up.

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Auto-Update Docs](https://img.shields.io/badge/docs-auto--updated-green.svg)](.github/workflows/update-readme.yml)

## ğŸ¯ About This Project

This repository provides a **complete, educational implementation** of Large Language Models, designed for:

- ğŸ“ **Students and researchers** learning about transformer architectures
- ğŸ’» **Developers** wanting to understand LLMs from first principles  
- ğŸ”¬ **Practitioners** looking for clean, well-documented reference implementations
- ğŸ¤ **Contributors** interested in advancing open-source ML education

### âœ¨ **Key Features**
- **Progressive complexity**: Each module builds naturally on previous concepts
- **Production-quality code**: Clean, typed, tested, and documented
- **Educational focus**: Extensive explanations, visualizations, and examples
- **Auto-updating documentation**: README stays synchronized with code changes
- **Interactive learning**: Jupyter notebooks for experimentation
- **Comprehensive testing**: Unit tests ensure code correctness

## ğŸ“š Documentation & Learning Resources

This repository includes comprehensive documentation to support your learning journey:

### ğŸ“– **Core Documentation**
- **[ğŸ¯ Learning Path](docs/learning_path.md)** - Step-by-step guide through all modules with time estimates, prerequisites, and study tips
- **[ğŸ“š Resources](docs/resources.md)** - Curated collection of papers, books, videos, and online resources
- **[ğŸ”§ Troubleshooting](docs/troubleshooting.md)** - Solutions to common implementation, training, and setup issues

### ğŸ¯ **Module-Specific Guides**
Each module includes its own README with:
- Learning objectives and key concepts
- Implementation details and explanations
- Usage examples and best practices
- Common issues and debugging tips

## ğŸ¯ Learning Path

This repository is organized into progressive modules. Here's the recommended learning sequence:

1. **Module 01: Tokenization** (`src/modules/01_tokenization/`)
2. **Module 02: Embeddings** (`src/modules/02_embeddings/`)
3. **Module 03: Attention** (`src/modules/03_attention/`)
4. **Module 04: Transformer Blocks** (`src/modules/04_transformer_blocks/`)
5. **Module 05: Gpt Model** (`src/modules/05_gpt_model/`)
6. **Module 06: Training** (`src/modules/06_training/`)
7. **Module 07: Inference** (`src/modules/07_inference/`)
8. **Module 08: Fine Tuning** (`src/modules/08_fine_tuning/`)

## ğŸš€ Quick Start

### **Prerequisites**
- Python 3.8+ installed
- Basic familiarity with PyTorch and neural networks
- 8GB+ RAM recommended (16GB+ for larger experiments)

### **Installation**
```bash
# Clone the repository
git clone https://github.com/alizeeshan-07/Build_LLM_From_Scratch.git
cd Build_LLM_From_Scratch

# Create and activate virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install the project in development mode
pip install -e .

# Verify installation
python examples/quick_start.py
```

### **First Steps**
1. ğŸ“– **Read the [Learning Path](docs/learning_path.md)** to understand the progression
2. ğŸƒâ€â™‚ï¸ **Start with Module 1**: `src/modules/01_tokenization/`
3. ğŸ“š **Follow along** with the module README and examples
4. ğŸ§ª **Experiment** with the Jupyter notebooks in `notebooks/`
5. â“ **Get help** from our [Troubleshooting Guide](docs/troubleshooting.md)

## ğŸ“– Module Overview

## Module 01: Tokenization

ğŸ“ **Location**: `src/modules/01_tokenization/`

# Module 1: Tokenization

## ğŸ¯ Overview

Tokenization is the foundational step in natural language processing where we convert raw text into numerical tokens that machine learning models can understand. This module implements a regex-based tokenizer that splits text using punctuation and whitespace patterns.

## ğŸ§  Core Concepts

### **What is Tokenization?**
Breaking down text into smaller units called "tokens" that computers can process:

```
Input:  "Hello, world!"
Tokens: ["Hello", ",", "world", "!"]  
IDs:    [45, 2, 123, 8]
```

### **Vocabulary**
A dictionary mapping tokens to unique integer IDs:

```python
vocab = {
    "hello": 0,
    "world": 1,
    ",": 2,
    "<|unk|>": 3,        # Unknown token
    "<|endoftext|>": 4   # End marker
}
```

### **Regex Pattern**
Our tokenizer uses `([,.:;?_!"()\']|--|\\s)` to split text:
- Captures punctuation as separate tokens
- Handles double dashes and whitespace
- Preserves important text structure

## ğŸ“ File Structure

```
src/modules/01_tokenization/
â”œâ”€â”€ simple_tokenizer.py     # Core TextTokenizer class
â”œâ”€â”€ build_vocabulary.py     # Vocabulary creation
â”œâ”€â”€ test.py                 # Testing script  
â”œâ”€â”€ utils.py                # Analysis tools
â””â”€â”€ README.md              # This guide
```

## ğŸ“„ Core Files Explained

### **`simple_tokenizer.py` - Main Implementation**

Contains the `TextTokenizer` class with two key methods:

#### **`encode(text)` - Text to Numbers**
1. **Split text** using regex pattern
2. **Clean tokens** (remove empty, strip whitespace)  
3. **Handle unknowns** (replace with `<|unk|>`)
4. **Convert to IDs** using vocabulary

```python
tokenizer = TextTokenizer(vocab)
ids = tokenizer.encode("Hello, world!")  # â†’ [45, 2, 123, 8]
```

#### **`decode(ids)` - Numbers to Text**
1. **Map IDs to tokens** using reverse vocabulary
2. **Join with spaces**
3. **Fix punctuation spacing** with regex

```python
text = tokenizer.decode([45, 2, 123, 8])  # â†’ "Hello, world!"
```

### **`build_vocabulary.py` - Vocabulary Creation**

Creates vocabularies from real text data:

#### **Main Pipeline: `create_full_vocabulary()`**
1. **Download text**: Gets "The Verdict" sample text
2. **Load and analyze**: Reads file, shows statistics
3. **Preprocess**: Splits text using same regex as tokenizer
4. **Build vocab**: Creates sorted token-to-ID mapping

```python
vocab = create_full_vocabulary()  # Downloads text and builds vocab
```

#### **Key Functions:**
- `download_sample_text()`: Downloads sample text file
- `preprocess_text()`: Splits text into tokens
- `build_vocabulary()`: Creates final vocab with special tokens

**Special tokens added:**
```python
all_tokens.extend(["<|endoftext|>", "<|unk|>"])
```

## ğŸ§ª How to Test

### **Run Complete Test Suite**
```bash
cd src/modules/01_tokenization/
python test.py
```

**What happens:**
1. Downloads sample text (~20K characters)
2. Builds vocabulary (~1,159 tokens)  
3. Creates tokenizer
4. Tests encoding/decoding on sample texts
5. Validates round-trip consistency
6. Shows vocabulary analysis

### **Manual Testing**
```python
from build_vocabulary import create_full_vocabulary
from simple_tokenizer import TextTokenizer

# Build vocabulary
vocab = create_full_vocabulary()

# Create tokenizer  
tokenizer = TextTokenizer(vocab)

# Test
text = "Hello, world!"
ids = tokenizer.encode(text)
decoded = tokenizer.decode(ids)

print(f"Original: {text}")
print(f"Token IDs: {ids}")  
print(f"Decoded: {decoded}")
print(f"Perfect match: {text == decoded}")
```

## âœ… Expected Results

**Successful test shows:**
```
Building vocabulary...
Vocabulary size: 1159
Creating tokenizer...

Test 1:
Original: Hello, world!
Token IDs: [789, 2, 456]
Decoded: Hello, world!
Perfect match: True

âœ… All tests completed successfully!
```

## ğŸ¯ Learning Outcomes

After this module, you should understand:
- âœ… How text becomes numerical tokens
- âœ… Vocabulary creation and management
- âœ… Handling unknown words with special tokens
- âœ… Round-trip encoding/decoding validation

## ğŸš€ Next Steps

1. **Test successfully**: Run `python test.py` without errors
2. **Experiment**: Try different texts and analyze results  
3. **Ready for Module 2**: Understanding embeddings

The foundation of your LLM is ready! ğŸ—ï¸

### ğŸ“‹ Files in this Module:

#### `simple_tokenizer.py`
Simple tokenizer implementation using regex-based splitting.

This module implements a basic tokenizer that splits text using regular expressions
and handles unknown tokens with a fallback strategy.

**Key Classes:**
- `TextTokenizer`: A simple regex-based tokenizer for text preprocessing

---

#### `test.py`
Simple test script for TextTokenizer.

This script tests the complete tokenization pipeline:
1. Download and build vocabulary
2. Create tokenizer
3. Test encoding/decoding on various texts

**Key Functions:**
- `test_basic_functionality()`: Test basic tokenizer encode/decode functionality
- `test_vocabulary_analysis()`: Test vocabulary analysis utilities
- `test_tokenization_analysis()`: Test tokenization frequency analysis
- `main()`: Run all tests

---

#### `utils.py`
Utilities specific to tokenization module.

This module provides analysis and visualization tools for understanding
tokenization behavior and vocabulary characteristics.

**Key Functions:**
- `analyze_vocabulary()`: Analyze and display vocabulary statistics
- `plot_token_frequencies()`: Plot most frequent tokens
- `compare_tokenizations()`: Compare two tokenizers side by side
- `tokenization_statistics()`: Calculate comprehensive tokenization statistics

---

#### `build_vocabulary.py`
Vocabulary building utilities for tokenization.

This module downloads sample text and creates vocabularies for tokenizer training.
Based on "The Verdict" text from the LLMs-from-scratch repository.

**Key Functions:**
- `download_sample_text()`: Download sample text file for vocabulary building
- `load_and_analyze_text()`: Load text file and display basic statistics
- `preprocess_text()`: Preprocess text using regex splitting
- `build_vocabulary()`: Build vocabulary dictionary from tokens
- `create_full_vocabulary()`: Complete pipeline to create vocabulary from sample text
- `save_vocabulary()`: Save vocabulary to file for later use
- `load_vocabulary()`: Load vocabulary from file

---

## Module 02: Embeddings

ğŸ“ **Location**: `src/modules/02_embeddings/`

*Module files will appear here as you implement them.*

## Module 03: Attention

ğŸ“ **Location**: `src/modules/03_attention/`

*Module files will appear here as you implement them.*

## Module 04: Transformer Blocks

ğŸ“ **Location**: `src/modules/04_transformer_blocks/`

*Module files will appear here as you implement them.*

## Module 05: Gpt Model

ğŸ“ **Location**: `src/modules/05_gpt_model/`

*Module files will appear here as you implement them.*

## Module 06: Training

ğŸ“ **Location**: `src/modules/06_training/`

*Module files will appear here as you implement them.*

## Module 07: Inference

ğŸ“ **Location**: `src/modules/07_inference/`

*Module files will appear here as you implement them.*

## Module 08: Fine Tuning

ğŸ“ **Location**: `src/modules/08_fine_tuning/`

*Module files will appear here as you implement them.*

## ğŸ› ï¸ Utilities & Tools

Common utilities and tools used across all modules:

### `cli.py`
Command-line interface for the Build LLM project.
Provides easy access to common operations.

### `data_utils.py`
**Key Functions:**
- `load_text_data()`: Load text from file
- `preprocess_text()`: Basic text preprocessing

### `metrics.py`
Evaluation metrics for language models.

### `visualization.py`
Visualization utilities for plotting attention weights, training curves, etc.

**Key Functions:**
- `plot_attention_weights()`: Plot attention weights as a heatmap
- `plot_training_curves()`: Plot training and validation loss curves
- `plot_token_embeddings()`: Plot token embeddings in 2D using dimensionality reduction

## ğŸ“ Project Structure

```
Build_LLM_From_Scratch/
â”œâ”€â”€ ğŸ“ src/
â”‚   â”œâ”€â”€ ğŸ“ modules/                    # Core LLM implementation modules
â”‚   â”‚   â”œâ”€â”€ ğŸ“ 01_tokenization/        # Text tokenization methods
â”‚   â”‚   â”œâ”€â”€ ğŸ“ 02_embeddings/          # Word and positional embeddings
â”‚   â”‚   â”œâ”€â”€ ğŸ“ 03_attention/           # Attention mechanisms
â”‚   â”‚   â”œâ”€â”€ ğŸ“ 04_transformer_blocks/  # Transformer architecture
â”‚   â”‚   â”œâ”€â”€ ğŸ“ 05_gpt_model/           # Complete GPT model
â”‚   â”‚   â”œâ”€â”€ ğŸ“ 06_training/            # Training procedures
â”‚   â”‚   â”œâ”€â”€ ğŸ“ 07_inference/           # Text generation
â”‚   â”‚   â””â”€â”€ ğŸ“ 08_fine_tuning/         # Model fine-tuning
â”‚   â””â”€â”€ ğŸ“ utils/                      # Common utilities
â”œâ”€â”€ ğŸ“ examples/                       # Usage examples and demos
â”œâ”€â”€ ğŸ“ notebooks/                      # Interactive Jupyter notebooks
â”œâ”€â”€ ğŸ“ tests/                          # Unit tests
â”œâ”€â”€ ğŸ“ docs/                           # Comprehensive documentation
â””â”€â”€ ğŸ“ data/                           # Sample datasets (git-ignored)
```

## ğŸ’¡ Usage Examples

### **Command Line Interface**
```bash
# Quick start demo
python examples/quick_start.py

# Train a small model
python examples/train_small_model.py

# Generate text
python examples/generate_text.py --prompt "The future of AI is"

# Use the CLI tool
build-llm train --config configs/small_model.yaml
build-llm generate --prompt "Hello, world!" --max-length 50
```

### **Python API**
```python
from src.modules.tokenization import SimpleTokenizer
from src.modules.gpt_model import GPTModel
from src.utils import plot_attention_weights

# Tokenize text
tokenizer = SimpleTokenizer()
tokens = tokenizer.encode("Hello, world!")

# Create and use model
model = GPTModel(vocab_size=10000, d_model=512)
output = model(tokens)

# Visualize attention
plot_attention_weights(attention_weights, tokens)
```

## ğŸ¤ Contributing

We welcome contributions! This project is designed to be educational and collaborative.

### **Ways to Contribute**
- ğŸ› **Bug reports** - Found an issue? Let us know!
- ğŸ’¡ **Feature requests** - Ideas for improvements?
- ğŸ“ **Documentation** - Help make explanations clearer
- ğŸ§ª **Testing** - Add tests for better reliability
- ğŸ“ **Educational content** - Notebooks, examples, tutorials
- ğŸ”§ **Code improvements** - Optimizations and clean-ups

### **Getting Started with Contributing**
1. Read our [Contributing Guidelines](CONTRIBUTING.md)
2. Check out [Good First Issues](https://github.com/alizeeshan-07/Build_LLM_From_Scratch/labels/good%20first%20issue)
3. Fork the repository and create a feature branch
4. Make your changes and add tests if applicable
5. Submit a pull request with a clear description

See [CONTRIBUTING.md](CONTRIBUTING.md) for detailed guidelines.

## ğŸ“Š Project Stats

- ğŸ **Language**: Python 3.8+
- ğŸ”¥ **Framework**: PyTorch 2.0+
- ğŸ“¦ **Modules**: 8 core learning modules
- ğŸ§ª **Tests**: Comprehensive test coverage
- ğŸ“š **Documentation**: Auto-generated and maintained
- ğŸ¤– **CI/CD**: Automated testing and documentation updates

## ğŸ™ Acknowledgments

This project is inspired by:
- ğŸ“– "Build a Large Language Model (From Scratch)" book
- ğŸ“ Stanford CS224N course materials
- ğŸ’» Andrej Karpathy's educational content
- ğŸ¤— Hugging Face's transformer implementations
- ğŸŒŸ The broader open-source ML community

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ“ Support & Contact

- ğŸ’¬ **Discussions**: [GitHub Discussions](https://github.com/alizeeshan-07/Build_LLM_From_Scratch/discussions)
- ğŸ› **Issues**: [GitHub Issues](https://github.com/alizeeshan-07/Build_LLM_From_Scratch/issues)
- ğŸ“§ **Email**: Open an issue for questions
- ğŸ“š **Documentation**: Check our [docs folder](docs/) for detailed guides

---

**â­ Star this repository if it helps you learn!**

*ğŸ“ This README is automatically updated when Python files are modified. Last updated: Auto-generated by GitHub Actions.*
